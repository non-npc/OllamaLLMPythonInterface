# OllamaLLMPythonInterface
with Ollama running locally (ollama serve) and with deepseek-coder-v2:latest model or other similar, you can now generate code on your desktop and it will allow you to select all the code you want to save locally.
